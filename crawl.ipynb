{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import warnings\n",
    "\n",
    "class SemanticScholarMetaDataExtractor():\n",
    "    def __init__(self):\n",
    "        self.API_ID = 'https://api.semanticscholar.org/v1/paper/arXiv:'\n",
    "\n",
    "    def get_response(self, paper_id):\n",
    "        paper_url = self.API_ID+paper_id\n",
    "        return urllib.request.urlopen(paper_url)\n",
    "    \n",
    "    def get_data_json(self, paper_id):\n",
    "        response = self.get_response(paper_id)\n",
    "        return json.loads(response.read())\n",
    "\n",
    "class ArXivPaper():\n",
    "    def __init__(self, paper):\n",
    "        self.paper = paper\n",
    "\n",
    "        self.essential_metadata_keys = {'abstract', 'arxivId', 'authors', 'citations', 'influentialCitationCount',\n",
    "                                        'doi', 'fieldsOfStudy', 'paperId', 'references',\n",
    "                                        'title', 'topics', 'url', 'venue', 'year'}\n",
    "\n",
    "        self.representational_info_keys = ['abstract', 'authors', 'url', 'year',\n",
    "                                           'fieldsOfStudy', 'numCitations', 'venue', 'numReferences']\n",
    "        \n",
    "        self.check_paper()\n",
    "        self.check_relevant_keys()\n",
    "        self.discard_non_influential_citations()\n",
    "        self.discard_non_influential_references()\n",
    "        self.discard_none_arxiv_references()\n",
    "        self.discard_none_arxiv_citations()\n",
    "\n",
    "    def check_paper(self):\n",
    "        if isinstance(self.paper, str):\n",
    "            warnings.warn(\"Paper not present in memory. Extracting Paper MetaData from Semantic Scholar!\")\n",
    "            metadata_extractor = SemanticScholarMetaDataExtractor()\n",
    "            self.paper = metadata_extractor.get_data_json(self.paper)\n",
    "        \n",
    "        elif not isinstance(self.paper, dict):\n",
    "            raise TypeError(\"Paper must be a Dict or an Arxiv Id\")\n",
    "    \n",
    "    def check_relevant_keys(self):\n",
    "        missing_keys = self.essential_metadata_keys.difference(self.paper.keys())\n",
    "        if not missing_keys == set():\n",
    "            error_message = \"The following essential keys are missing from the paper: \" + \\\n",
    "                            \", \".join(missing_keys)\n",
    "            raise KeyError(error_message)\n",
    "        \n",
    "        self.paper['numCitations'] = len(self.paper['citations'])\n",
    "        self.paper['numReferences'] = len(self.paper['references'])\n",
    "\n",
    "    def discard_non_influential_citations(self):\n",
    "        self.paper['citations'] = list(filter(lambda i: i['isInfluential'] is True, self.paper['citations']))\n",
    "\n",
    "    def discard_none_arxiv_references(self):\n",
    "        self.paper['references'] = list(filter(lambda i: i['arxivId'] is not None, self.paper['references'])) \n",
    "\n",
    "    def discard_none_arxiv_citations(self):\n",
    "        self.paper['citations'] = list(filter(lambda i: i['arxivId'] is not None, self.paper['citations']))\n",
    "\n",
    "    def discard_non_influential_references(self):\n",
    "        self.paper['references'] = list(filter(lambda i: i['isInfluential'] is True, self.paper['references'])) \n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.paper[key]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr = f\"Paper Title: {self.__getitem__('title')} \\n\\n\"\n",
    "        for idx, key in enumerate(self.representational_info_keys):\n",
    "            if key == 'abstract':\n",
    "                repr += f\"{idx+1}) {'Abstract'}: \\n{self.__getitem__(key)} \\n\\n\"\n",
    "                continue\n",
    "            if key == 'authors':\n",
    "                repr += f\"{idx+1}) {'Authors'}:\\n\"\n",
    "                authors = self.__getitem__(key)\n",
    "                for i, author in enumerate(authors):\n",
    "                    repr += f\"\\t{i+1}) {'Name'}: {author.__getitem__('name')}\\n\"\n",
    "                    repr += f\"\\t{'URL'}: {author.__getitem__('url')}\\n\"\n",
    "                    repr +=\"\\n\"\n",
    "                continue\n",
    "            repr += f\"{idx+1}) {key}: {self.__getitem__(key)} \\n\\n\"\n",
    "        return(repr)\n",
    "    \n",
    "    def get_top_k_citations_information(self, k:int):\n",
    "        if k > self.__getitem__('numCitations'):\n",
    "             warnings.warn(f\"Total citations are {self.__getitem__('numCitations')}. Retrieving all citations\")\n",
    "             k = self.__getitem__('numCitations')\n",
    "\n",
    "        citations = []\n",
    "        all_citations = self.__getitem__('citations')\n",
    "\n",
    "        info_keys = ['arxivId', 'authors', 'title', 'url','venue', 'year']\n",
    "\n",
    "        i=0\n",
    "        while i < k:\n",
    "            citation = all_citations[i]        \n",
    "            citation = {key:val for key, val in citation.items() if key in info_keys}\n",
    "            citations.append(citation)\n",
    "            i+=1\n",
    "\n",
    "        return citations\n",
    "\n",
    "    def get_top_k_references_information(self, k:int):\n",
    "        if k > self.__getitem__('numReferences'):\n",
    "             warnings.warn(f\"Total references are {self.__getitem__('numReferences')}. Retrieving all references\")\n",
    "             k = self.__getitem__('numReferences')\n",
    "\n",
    "        references = []\n",
    "        all_references = self.__getitem__('references')\n",
    "\n",
    "        info_keys = ['arxivId', 'authors', 'title', 'url','venue', 'year']\n",
    "\n",
    "        i=0\n",
    "        while i < k:\n",
    "            reference = all_references[i]\n",
    "            reference = {key:val for key, val in reference.items() if key in info_keys}\n",
    "            references.append(reference)\n",
    "            i+=1\n",
    "\n",
    "        return references\n",
    "\n",
    "    def get_top_k_references_metadata(self, k:int):\n",
    "        reference_papers = []\n",
    "        references = self.get_top_k_references_information(k)\n",
    "\n",
    "        for i in range(len(references)):\n",
    "            reference_papers.append(ArXivPaper(references[i]['arxivId']))\n",
    "\n",
    "        return reference_papers\n",
    "\n",
    "    def get_top_k_citations_metadata(self, k:int):\n",
    "        citation_papers = []\n",
    "        citations = self.get_top_k_citations_information(k)\n",
    "\n",
    "        for i in range(len(citations)):\n",
    "            citation_papers.append(ArXivPaper(citations[i]['arxivId']))\n",
    "\n",
    "        return citation_papers\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = ArXivPaper(\"1806.07366\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = paper.get_top_k_references_information(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'arxivId': '1708.00065',\n",
       "  'authors': [{'authorId': None, 'name': 'Yang Li'},\n",
       "   {'authorId': '145585757', 'name': 'Nan Du'},\n",
       "   {'authorId': '1751569', 'name': 'S. Bengio'}],\n",
       "  'title': 'Time-Dependent Representation for Neural Event Sequence Prediction',\n",
       "  'url': 'https://www.semanticscholar.org/paper/ec7bab52b2220a6cad410dd82b3fbe140d2196f0',\n",
       "  'venue': 'ICLR',\n",
       "  'year': 2018},\n",
       " {'arxivId': '1606.04130',\n",
       "  'authors': [{'authorId': '32219137', 'name': 'Zachary Chase Lipton'},\n",
       "   {'authorId': '2107807', 'name': 'David C. Kale'},\n",
       "   {'authorId': '144616817', 'name': 'R. Wetzel'}],\n",
       "  'title': 'Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series',\n",
       "  'url': 'https://www.semanticscholar.org/paper/562f33611cdc0d8ed6609aa09f153e6238d5409e',\n",
       "  'venue': 'MLHC',\n",
       "  'year': 2016},\n",
       " {'arxivId': '1505.05770',\n",
       "  'authors': [{'authorId': '1748523', 'name': 'Danilo Jimenez Rezende'},\n",
       "   {'authorId': '14594344', 'name': 'S. Mohamed'}],\n",
       "  'title': 'Variational Inference with Normalizing Flows',\n",
       "  'url': 'https://www.semanticscholar.org/paper/0f899b92b7fb03b609fee887e4b6f3b633eaf30d',\n",
       "  'venue': 'ICML',\n",
       "  'year': 2015}]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_papers = paper.get_top_k_references_metadata(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Paper Title: Time-Dependent Representation for Neural Event Sequence Prediction \n",
       " \n",
       " 1) Abstract: \n",
       " Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: Yang Li\n",
       " \tURL: None\n",
       " \n",
       " \t2) Name: Nan Du\n",
       " \tURL: https://www.semanticscholar.org/author/145585757\n",
       " \n",
       " \t3) Name: S. Bengio\n",
       " \tURL: https://www.semanticscholar.org/author/1751569\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/ec7bab52b2220a6cad410dd82b3fbe140d2196f0 \n",
       " \n",
       " 4) year: 2018 \n",
       " \n",
       " 5) fieldsOfStudy: ['Computer Science', 'Mathematics'] \n",
       " \n",
       " 6) numCitations: 28 \n",
       " \n",
       " 7) venue: ICLR \n",
       " \n",
       " 8) numReferences: 25 \n",
       " ,\n",
       " Paper Title: Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series \n",
       " \n",
       " 1) Abstract: \n",
       " We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the intensive care unit (ICU) of a major urban medical center, our data consists of multivariate time series of observations. The data is irregularly sampled, leading to missingness patterns in re-sampled sequences. In this work, we show the remarkable ability of RNNs to make effective use of binary indicators to directly model missing data, improving AUC and F1 significantly. However, while RNNs can learn arbitrary functions of the missing data and observations, linear models can only learn substitution values. For linear models and MLPs, we show an alternative strategy to capture this signal. Additionally, we evaluate LSTMs, MLPs, and linear models trained on missingness patterns only, showing that for several diseases, what tests are run can be more predictive than the results themselves. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: Zachary Chase Lipton\n",
       " \tURL: https://www.semanticscholar.org/author/32219137\n",
       " \n",
       " \t2) Name: David C. Kale\n",
       " \tURL: https://www.semanticscholar.org/author/2107807\n",
       " \n",
       " \t3) Name: R. Wetzel\n",
       " \tURL: https://www.semanticscholar.org/author/144616817\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/562f33611cdc0d8ed6609aa09f153e6238d5409e \n",
       " \n",
       " 4) year: 2016 \n",
       " \n",
       " 5) fieldsOfStudy: ['Computer Science', 'Mathematics'] \n",
       " \n",
       " 6) numCitations: 108 \n",
       " \n",
       " 7) venue: MLHC \n",
       " \n",
       " 8) numReferences: 26 \n",
       " ,\n",
       " Paper Title: Variational Inference with Normalizing Flows \n",
       " \n",
       " 1) Abstract: \n",
       " The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: Danilo Jimenez Rezende\n",
       " \tURL: https://www.semanticscholar.org/author/1748523\n",
       " \n",
       " \t2) Name: S. Mohamed\n",
       " \tURL: https://www.semanticscholar.org/author/14594344\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/0f899b92b7fb03b609fee887e4b6f3b633eaf30d \n",
       " \n",
       " 4) year: 2015 \n",
       " \n",
       " 5) fieldsOfStudy: ['Mathematics', 'Computer Science'] \n",
       " \n",
       " 6) numCitations: 1225 \n",
       " \n",
       " 7) venue: ICML \n",
       " \n",
       " 8) numReferences: 40 \n",
       " ]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "references_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in paper['citations']:\n",
    "    if i['isInfluential'] == True:\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'arxivId': None,\n",
       " 'authors': [{'authorId': '104314859', 'name': 'Fred Daum'},\n",
       "  {'authorId': '50535618', 'name': 'J. Huang'},\n",
       "  {'authorId': '9130376', 'name': 'A. Noushin'}],\n",
       " 'doi': '10.1117/12.2517980',\n",
       " 'intent': ['background'],\n",
       " 'isInfluential': False,\n",
       " 'paperId': 'd13739de9b7e22eea9ff03c23d322817c14bdfd8',\n",
       " 'title': \"Extremely deep Bayesian learning with Gromov's method\",\n",
       " 'url': 'https://www.semanticscholar.org/paper/d13739de9b7e22eea9ff03c23d322817c14bdfd8',\n",
       " 'venue': 'Defense + Commercial Sensing',\n",
       " 'year': 2019}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "paper.paper['citations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}