{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import warnings\n",
    "\n",
    "class SemanticScholarMetaDataExtractor():\n",
    "    def __init__(self):\n",
    "        self.API_ID = 'https://api.semanticscholar.org/v1/paper/arXiv:'\n",
    "\n",
    "    def get_response(self, paper_id):\n",
    "        paper_url = self.API_ID+paper_id\n",
    "        return urllib.request.urlopen(paper_url)\n",
    "    \n",
    "    def get_data_json(self, paper_id):\n",
    "        response = self.get_response(paper_id)\n",
    "        return json.loads(response.read())\n",
    "\n",
    "class ArXivPaper():\n",
    "    def __init__(self, paper):\n",
    "        self.paper = paper\n",
    "\n",
    "        self.essential_metadata_keys = {'abstract', 'arxivId', 'authors', 'citations', 'influentialCitationCount',\n",
    "                                        'doi', 'fieldsOfStudy', 'paperId', 'references',\n",
    "                                        'title', 'topics', 'url', 'venue', 'year'}\n",
    "\n",
    "        self.representational_info_keys = ['abstract', 'authors', 'url', 'year',\n",
    "                                           'fieldsOfStudy', 'numCitations', 'venue', 'numReferences']\n",
    "        \n",
    "        self.check_paper()\n",
    "        self.check_relevant_keys()\n",
    "        self.discard_non_influential_citations()\n",
    "        self.discard_non_influential_references()\n",
    "        self.discard_none_arxiv_references()\n",
    "        self.discard_none_arxiv_citations()\n",
    "        self.set_num_references()\n",
    "        self.set_num_citations()\n",
    "\n",
    "    def check_paper(self):\n",
    "        if isinstance(self.paper, str):\n",
    "            warnings.warn(\"Paper not present in memory. Extracting Paper MetaData from Semantic Scholar!\")\n",
    "            metadata_extractor = SemanticScholarMetaDataExtractor()\n",
    "            self.paper = metadata_extractor.get_data_json(self.paper)\n",
    "        \n",
    "        elif not isinstance(self.paper, dict):\n",
    "            raise TypeError(\"Paper must be a Dict or an Arxiv Id\")\n",
    "    \n",
    "    def check_relevant_keys(self):\n",
    "        missing_keys = self.essential_metadata_keys.difference(self.paper.keys())\n",
    "        if not missing_keys == set():\n",
    "            error_message = \"The following essential keys are missing from the paper: \" + \\\n",
    "                            \", \".join(missing_keys)\n",
    "            raise KeyError(error_message)\n",
    "        \n",
    "\n",
    "    def discard_non_influential_citations(self):\n",
    "        self.paper['citations'] = list(filter(lambda i: i['isInfluential'] is True, self.paper['citations']))\n",
    "\n",
    "    def discard_none_arxiv_references(self):\n",
    "        self.paper['references'] = list(filter(lambda i: i['arxivId'] is not None, self.paper['references'])) \n",
    "\n",
    "    def discard_none_arxiv_citations(self):\n",
    "        self.paper['citations'] = list(filter(lambda i: i['arxivId'] is not None, self.paper['citations']))\n",
    "\n",
    "    def discard_non_influential_references(self):\n",
    "        self.paper['references'] = list(filter(lambda i: i['isInfluential'] is True, self.paper['references'])) \n",
    "\n",
    "    def set_num_references(self):\n",
    "        self.paper['numReferences'] = len(self.paper['references'])\n",
    "\n",
    "    def set_num_citations(self):\n",
    "        self.paper['numCitations'] = len(self.paper['citations'])\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.paper[key]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr = f\"Paper Title: {self.__getitem__('title')} \\n\\n\"\n",
    "        for idx, key in enumerate(self.representational_info_keys):\n",
    "            if key == 'abstract':\n",
    "                repr += f\"{idx+1}) {'Abstract'}: \\n{self.__getitem__(key)} \\n\\n\"\n",
    "                continue\n",
    "            if key == 'authors':\n",
    "                repr += f\"{idx+1}) {'Authors'}:\\n\"\n",
    "                authors = self.__getitem__(key)\n",
    "                for i, author in enumerate(authors):\n",
    "                    repr += f\"\\t{i+1}) {'Name'}: {author.__getitem__('name')}\\n\"\n",
    "                    repr += f\"\\t{'URL'}: {author.__getitem__('url')}\\n\"\n",
    "                    repr +=\"\\n\"\n",
    "                continue\n",
    "            repr += f\"{idx+1}) {key}: {self.__getitem__(key)} \\n\\n\"\n",
    "        return(repr)\n",
    "    \n",
    "    def get_top_k_citations_information(self, k:int):\n",
    "        if k > self.__getitem__('numCitations'):\n",
    "             warnings.warn(f\"Total citations are {self.__getitem__('numCitations')}. Retrieving all citations\")\n",
    "             k = self.__getitem__('numCitations')\n",
    "\n",
    "        citations = []\n",
    "        all_citations = self.__getitem__('citations')\n",
    "\n",
    "        info_keys = ['arxivId', 'authors', 'title', 'url','venue', 'year']\n",
    "\n",
    "        i=0\n",
    "        while i < k:\n",
    "            citation = all_citations[i]        \n",
    "            citation = {key:val for key, val in citation.items() if key in info_keys}\n",
    "            citations.append(citation)\n",
    "            i+=1\n",
    "\n",
    "        return citations\n",
    "\n",
    "    def get_top_k_references_information(self, k:int):\n",
    "        if k > self.__getitem__('numReferences'):\n",
    "             warnings.warn(f\"Total references are {self.__getitem__('numReferences')}. Retrieving all references\")\n",
    "             k = self.__getitem__('numReferences')\n",
    "\n",
    "        references = []\n",
    "        all_references = self.__getitem__('references')\n",
    "\n",
    "        info_keys = ['arxivId', 'authors', 'title', 'url','venue', 'year']\n",
    "\n",
    "        i=0\n",
    "        while i < k:\n",
    "            reference = all_references[i]\n",
    "            reference = {key:val for key, val in reference.items() if key in info_keys}\n",
    "            references.append(reference)\n",
    "            i+=1\n",
    "\n",
    "        return references\n",
    "\n",
    "    def get_top_k_references_metadata(self, k:int):\n",
    "        reference_papers = []\n",
    "        references = self.get_top_k_references_information(k)\n",
    "\n",
    "        for i in range(len(references)):\n",
    "            reference_papers.append(ArXivPaper(references[i]['arxivId']))\n",
    "\n",
    "        return reference_papers\n",
    "\n",
    "    def get_top_k_citations_metadata(self, k:int):\n",
    "        citation_papers = []\n",
    "        citations = self.get_top_k_citations_information(k)\n",
    "\n",
    "        for i in range(len(citations)):\n",
    "            citation_papers.append(ArXivPaper(citations[i]['arxivId']))\n",
    "\n",
    "        return citation_papers\n",
    "    \n",
    "\n",
    "class GraphNode():\n",
    "    def __init__(self, paper:ArXivPaper, num_citations:int=10, num_references:int=10):\n",
    "        self.paper = paper \n",
    "        self.num_citations = num_citations\n",
    "        self.num_references = num_references\n",
    "        self.citation_children = None\n",
    "        self.reference_children = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.paper.__repr__()\n",
    "\n",
    "    def is_reference_leaf(self):\n",
    "         return self.paper['numReferences'] == 0\n",
    "\n",
    "    def is_citation_leaf(self):\n",
    "         return self.paper['numCitations'] == 0\n",
    "    \n",
    "    def get_citation_children(self):\n",
    "        if not self.is_citation_leaf():\n",
    "            self.citation_children = self.paper.get_top_k_citations_information(self.num_citations)\n",
    "\n",
    "    def get_reference_children(self):\n",
    "        if not self.is_reference_leaf():\n",
    "            self.reference_children = self.paper.get_top_k_references_information(self.num_references)\n",
    "\n",
    "    def get_citation_children_metadata(self):\n",
    "        essential_metadata_keys = ['abstract', 'arxivId', 'numCitations', 'title']\n",
    "        citation_children_metadata = self.paper.get_top_k_citations_metadata(self.num_citations)\n",
    "        \n",
    "        for idx, child in enumerate(citation_children_metadata):\n",
    "            citation_children_metadata[idx] = {key:val for key, val in child.paper.items() if key in essential_metadata_keys}\n",
    "        \n",
    "        return citation_children_metadata\n",
    "\n",
    "    def get_reference_children_metadata(self):\n",
    "        essential_metadata_keys = ['abstract', 'arxivId', 'numReferences', 'title']\n",
    "        reference_children_metadata = self.paper.get_top_k_references_metadata(self.num_references)\n",
    "\n",
    "        for idx, child in enumerate(reference_children_metadata):\n",
    "            reference_children_metadata[idx] = {key:val for key, val in child.paper.items() if key in essential_metadata_keys}\n",
    "\n",
    "        return reference_children_metadata\n",
    "\n",
    "class Graph():\n",
    "    def __init__(self, root:GraphNode):\n",
    "        self.root = root\n",
    "        self.citation_branch = []\n",
    "        self.reference_branch = []\n",
    "\n",
    "    def get_root_citations(self):\n",
    "        return self.root.get_citation_children_metadata()\n",
    "\n",
    "    def get_root_references(self):\n",
    "        return self.root.get_reference_children_metadata()\n",
    "\n",
    "    def build_citations_subtree(self, arxiv_idx:str):\n",
    "        node = GraphNode(ArXivPaper(arxiv_idx))\n",
    "        self.citation_branch.append(node)\n",
    "        return node.get_citation_children_metadata()\n",
    "\n",
    "    def build_references_subtree(self, arxiv_idx:str):\n",
    "        node = GraphNode(ArXivPaper(arxiv_idx))\n",
    "        self.references_branch.append(node)\n",
    "        return node.get_reference_children_metadata()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = GraphNode(ArXivPaper(\"1806.07366\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'abstract': 'Over the past few years, deep learning has risen to the foreground as a topic of massive interest, mainly as a result of successes obtained in solving large-scale image processing tasks. There are multiple challenging mathematical problems involved in applying deep learning: most deep learning methods require the solution of hard optimisation problems, and a good understanding of the tradeoff between computational effort, amount of data and model complexity is required to successfully design a deep learning approach for a given problem. A large amount of progress made in deep learning has been based on heuristic explorations, but there is a growing effort to mathematically understand the structure in existing deep learning methods and to systematically design new deep learning methods to preserve certain types of structure in deep learning. In this article, we review a number of these directions: some deep neural networks can be understood as discretisations of dynamical systems, neural networks can be designed to have desirable properties such as invertibility or group equivariance, and new algorithmic frameworks based on conformal Hamiltonian systems and Riemannian manifolds to solve the optimisation problems have been proposed. We conclude our review of each of these topics by discussing some open problems that we consider to be interesting directions for future research.',\n",
       "  'arxivId': '2006.03364',\n",
       "  'title': 'Structure preserving deep learning',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Neural differential equations are a promising new member in the neural network family. They show the potential of differential equations for time-series data analysis. In this paper, the strength of the ordinary differential equation (ODE) is explored with a new extension. The main goal of this work is to answer the following questions: (i) can ODE be used to redefine the existing neural network model? (ii) can Neural ODEs solve the irregular sampling rate challenge of existing neural network models for a continuous time series, i.e., length and dynamic nature, (iii) how to reduce the training and evaluation time of existing Neural ODE systems? This work leverages the mathematical foundation of ODEs to redesign traditional RNNs such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The main contribution of this paper is to illustrate the design of two new ODE-based RNN models (GRU-ODE model and LSTM-ODE) which can compute the hidden state and cell state at any point of time using an ODE solver. These models reduce the computation overhead of hidden state and cell state by a vast amount. The performance evaluation of these two new models for learning continuous time series with irregular sampling rate is then demonstrated. Experiments show that these new ODE based RNN models require less training time than Latent ODEs and conventional Neural ODEs. They can achieve higher accuracy quickly, and the design of the neural network is more straightforward than the previous neural ODE systems.',\n",
       "  'arxivId': '2005.09807',\n",
       "  'title': 'Neural Ordinary Differential Equation based Recurrent Neural Network Model',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Model noise is known to have detrimental effects on neural networks, such as training instability and predictive distributions with non-calibrated uncertainty properties. These factors set bottlenecks on the expressive potential of Neural Stochastic Differential Equations (NSDEs), a model family that employs neural nets on both drift and diffusion functions. We introduce a novel algorithm that solves a generic NSDE using only deterministic approximation methods. Given a discretization, we estimate the marginal distribution of the Ito process implied by the NSDE using a recursive scheme to propagate deterministic approximations of the statistical moments across time steps. The proposed algorithm comes with theoretical guarantees on numerical stability and convergence to the true solution, enabling its computational use for robust, accurate, and efficient prediction of long sequences. We observe our novel algorithm to behave interpretably on synthetic setups and to improve the state of the art on two challenging real-world tasks.',\n",
       "  'arxivId': '2006.08973',\n",
       "  'title': 'Deterministic Inference of Neural Stochastic Differential Equations',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Compressing deep neural networks (DNNs) is important for real-world applications operating on resource-constrained devices. However, it is not straightforward to change the model size (i.e., computational complexity) once training and compression are completed, calling for retraining to construct models suitable for different devices. In this paper, we propose a novel method, Decomposable-Net (the network decomposable in any size), which allows flexible changes to model size without retraining. We decompose weight matrices in the DNNs via singular value decomposition and adjust ranks according to the target model size. Unlike the existing methods, (1) we propose a learning method that explicitly minimizes losses for both of full-rank and low-rank networks, which is designed not only to maintain the performance of a full-rank network but also to improve multiple low-rank networks in a single model. (2) We also provide a mathematical analysis for the scalability of the approximation error with respect to the rank in each layer. Moreover, on the basis of the analysis, (3) we introduce a simple criterion for rank selection that effectively suppresses approximation error. In experiments on image-classification tasks on CIFAR-10/100 and ImageNet datasets, Decomposable-Net yields favorable performance in a broader range of compressed models. In particular, Decomposable-Net achieves the top-1 accuracy of $73.2\\\\%$ with $0.27\\\\times$MACs on the ImageNet classification task with ResNet-50, compared to low-rank tensor (Tucker) decomposition ($67.4\\\\% / 0.30\\\\times$) and universally slimmable networks ($70.6\\\\% / 0.26\\\\times$).',\n",
       "  'arxivId': '1910.13141',\n",
       "  'title': 'Decomposable-Net: Scalable Low-Rank Compression for Neural Networks',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'We introduce SchrodingeRNN, a quantum inspired generative model for raw audio. Audio data is wave-like and is sampled from a continuous signal. Although generative modelling of raw audio has made great strides lately, relational inductive biases relevant to these two characteristics are mostly absent from models explored to date. Quantum Mechanics is a natural source of probabilistic models of wave behaviour. Our model takes the form of a stochastic Schrodinger equation describing the continuous time measurement of a quantum system, and is equivalent to the continuous Matrix Product State (cMPS) representation of wavefunctions in one dimensional many-body systems. This constitutes a deep autoregressive architecture in which the systems state is a latent representation of the past observations. We test our model on synthetic data sets of stationary and non-stationary signals. This is the first time cMPS are used in machine learning.',\n",
       "  'arxivId': '1911.11879',\n",
       "  'title': 'SchrödingeRNN: Generative Modeling of Raw Audio as a Continuously Observed Quantum State',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Predicting future states or actions of a given system remains a fundamental, yet unsolved challenge of intelligence, especially in the scope of complex and nondeterministic scenarios, such as modeling behavior of humans. Existing approaches provide results under strong assumptions concerning unimodality of future states, or, at best, assuming specific probability distributions that often poorly fit to real-life conditions. In this work we introduce a robust and flexible probabilistic framework that allows to model future predictions with virtually no constrains regarding the modality or underlying probability distribution. To achieve this goal, we leverage a hypernetwork architecture and train a continuous normalizing flow model. The resulting method dubbed RegFlow achieves state-of-the-art results on several benchmark datasets, outperforming competing approaches by a significant margin.',\n",
       "  'arxivId': '2011.14620',\n",
       "  'title': 'RegFlow: Probabilistic Flow-based Regression for Future Prediction',\n",
       "  'numCitations': 0},\n",
       " {'abstract': \"A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE's dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow's Jacobian can be obtained by integrating the trace of the dynamics' Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to a state-of-the-art CNF while on average requiring one-fourth of the number of weights with 19x speedup in training time and 28x speedup in inference.\",\n",
       "  'arxivId': '2006.00104',\n",
       "  'title': 'OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport',\n",
       "  'numCitations': 1},\n",
       " {'abstract': 'Fueled by the availability of more data and computing power, recent breakthroughs in cloud-based machine learning (ML) have transformed every aspect of our lives from face recognition and medical diagnosis to natural language processing. However, classical ML exerts severe demands in terms of energy, memory, and computing resources, limiting their adoption for resource-constrained edge devices. The new breed of intelligent devices and high-stake applications (drones, augmented/virtual reality, autonomous systems, and so on) requires a novel paradigm change calling for distributed, low-latency and reliable ML at the wireless network edge (referred to as edge ML). In edge ML, training data are unevenly distributed over a large number of edge nodes, which have access to a tiny fraction of the data. Moreover, training and inference are carried out collectively over wireless links, where edge devices communicate and exchange their learned models (not their private data). In a first of its kind, this article explores the key building blocks of edge ML, different neural network architectural splits and their inherent tradeoffs, as well as theoretical and technical enablers stemming from a wide range of mathematical disciplines. Finally, several case studies pertaining to various high-stake applications are presented to demonstrate the effectiveness of edge ML in unlocking the full potential of 5G and beyond.',\n",
       "  'arxivId': '1812.02858',\n",
       "  'title': 'Wireless Network Intelligence at the Edge',\n",
       "  'numCitations': 3},\n",
       " {'abstract': 'Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.',\n",
       "  'arxivId': '1902.09689',\n",
       "  'title': 'AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks',\n",
       "  'numCitations': 7},\n",
       " {'abstract': 'We use neural ordinary differential equations to formulate a variant of the Transformer that is depth-adaptive in the sense that an input-dependent number of time steps is taken by the ordinary differential equation solver. Our goal in proposing the N-ODE Transformer is to investigate whether its depth-adaptivity may aid in overcoming some specific known theoretical limitations of the Transformer in handling nonlocal effects. Specifically, we consider the simple problem of determining the parity of a binary sequence, for which the standard Transformer has known limitations that can only be overcome by using a sufficiently large number of layers or attention heads. We find, however, that the depth-adaptivity of the N-ODE Transformer does not provide a remedy for the inherently nonlocal nature of the parity problem, and provide explanations for why this is so. Next, we pursue regularization of the N-ODE Transformer by penalizing the arclength of the ODE trajectories, but find that this fails to improve the accuracy or efficiency of the N-ODE Transformer on the challenging parity problem. We suggest future avenues of research for modifications and extensions of the N-ODE Transformer that may lead to improved accuracy and efficiency for sequence modelling tasks such as neural machine translation.',\n",
       "  'arxivId': '2010.11358',\n",
       "  'title': 'N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using Neural Ordinary Differential Equations',\n",
       "  'numCitations': 0}]"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "g.get_root_citations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'abstract': 'Random Recurrent Neural Networks (RRNN) are the simplest recurrent networks to model and extract features from sequential data. The simplicity however comes with a price; RRNN are known to be susceptible to diminishing/exploding gradient problem when trained with gradient-descent based optimization. To enhance robustness of RRNN, alternative training approaches have been proposed. Specifically, FORCE learning approach proposed a recursive least squares alternative to train RRNN and was shown to be applicable even for the challenging task of target-learning, where the network is tasked with generating dynamic patterns with no guiding input. While FORCE training indicates that solving target-learning is possible, it appears to be effective only in a specific regime of network dynamics (edge-of-chaos). We thereby investigate whether initialization of RRNN connectivity according to a tailored distribution can guarantee robust FORCE learning. We are able to generate such distribution by inference of four generating principles constraining the spectrum of the network Jacobian to remain in stability region. This initialization along with FORCE learning provides a robust training method, i.e., Robust-FORCE (R-FORCE). We validate R-FORCE performance on various target functions for a wide range of network configurations and compare with alternative methods. Our experiments indicate that R-FORCE facilitates significantly more stable and accurate target-learning for a wide class of RRNN. Such stability becomes critical in modeling multi-dimensional sequences as we demonstrate on modeling time-series of human body joints during physical movements.',\n",
       "  'arxivId': '2003.11660',\n",
       "  'title': 'R-FORCE: Robust Learning for Random Recurrent Neural Networks',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Recurrent neural networks (RNNs) are particularly well-suited for modeling long-term dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, well-chosen parametric constraints, and skip-connections, we develop a novel perspective that seeks to evolve the hidden state on the equilibrium manifold of an ordinary differential equation (ODE). We propose a family of novel RNNs, namely {\\\\em Equilibriated Recurrent Neural Networks} (ERNNs) that overcome the gradient decay or explosion effect and lead to recurrent models that evolve on the equilibrium manifold. We show that equilibrium points are stable, leading to fast convergence of the discretized ODE to fixed points. Furthermore, ERNNs account for long-term dependencies, and can efficiently recall informative aspects of data from the distant past. We show that ERNNs achieve state-of-the-art accuracy on many challenging data sets with 3-10x speedups, 1.5-3x model size reduction, and with similar prediction cost relative to vanilla RNNs.',\n",
       "  'arxivId': '1908.08574',\n",
       "  'title': 'RNNs Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.',\n",
       "  'arxivId': '1909.13334',\n",
       "  'title': 'Symplectic Recurrent Neural Networks',\n",
       "  'numCitations': 6},\n",
       " {'abstract': 'Recurrent neural networks (RNNs) have gained a great deal of attention in solving sequential learning problems. The learning of long-term dependencies, however, remains challenging due to the problem of a vanishing or exploding hidden states gradient. By exploring further the recently established connections between RNNs and dynamical systems we propose a novel RNN architecture, which we call a Hamiltonian recurrent neural network (Hamiltonian RNN), based on a symplectic discretization of an appropriately chosen Hamiltonian system. The key benefit of this approach is that the corresponding RNN inherits the favorable long time properties of the Hamiltonian system, which in turn allows us to control the hidden states gradient with a hyperparameter of the Hamiltonian RNN architecture. This enables us to handle sequential learning problems with arbitrary sequence lengths, since for a range of values of this hyperparameter the gradient neither vanishes nor explodes. Additionally, we provide a heuristic for the optimal choice of the hyperparameter, which we use in our numerical simulations to illustrate that the Hamiltonian RNN is able to outperform other state-of-the-art RNNs without the need of computationally intensive hyperparameter optimization.',\n",
       "  'arxivId': '1911.05035',\n",
       "  'title': 'Constructing Gradient Controllable Recurrent Neural Networks Using Hamiltonian Dynamics',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Established recurrent neural networks are well-suited to solve a wide variety of prediction tasks involving discrete sequences. However, they do not perform as well in the task of dynamical system identification, when dealing with observations from continuous variables that are unevenly sampled in time, for example due to missing observations. We show how such neural sequence models can be adapted to deal with variable step sizes in a natural way. In particular, we introduce a time-aware and stationary extension of existing models (including the Gated Recurrent Unit) that allows them to deal with unevenly sampled system observations by adapting to the observation times, while facilitating higher-order temporal behavior. We discuss the properties and demonstrate the validity of the proposed approach, based on samples from two industrial input/output processes.',\n",
       "  'arxivId': '1911.09431',\n",
       "  'title': 'System Identification with Time-Aware Neural Sequence Models',\n",
       "  'numCitations': 0},\n",
       " {'abstract': \"To understand the fundamental trade-offs between training stability, temporal dynamics and architectural complexity of recurrent neural networks~(RNNs), we directly analyze RNN architectures using numerical methods of ordinary differential equations~(ODEs). We define a general family of RNNs--the ODERNNs--by relating the composition rules of RNNs to integration methods of ODEs at discrete time steps. We show that the degree of RNN's functional nonlinearity $n$ and the range of its temporal memory $t$ can be mapped to the corresponding stage of Runge-Kutta recursion and the order of time-derivative of the ODEs. We prove that popular RNN architectures, such as LSTM and URNN, fit into different orders of $n$-$t$-ODERNNs. This exact correspondence between RNN and ODE helps us to establish the sufficient conditions for RNN training stability and facilitates more flexible top-down designs of new RNN architectures using large varieties of toolboxes from numerical integration of ODEs. We provide such an example: Quantum-inspired Universal computing Neural Network~(QUNN), which reduces the required number of training parameters from polynomial in both data length and temporal memory length to only linear in temporal memory length.\",\n",
       "  'arxivId': '1904.12933',\n",
       "  'title': 'Recurrent Neural Networks in the Eye of Differential Equations',\n",
       "  'numCitations': 0},\n",
       " {'abstract': \"Differential equations are a natural choice for modeling recurrent neural networks because they can be viewed as dynamical systems with a driving input. In this work, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form simplifies stability analysis, which enables us to provide an asymptotic stability guarantee. Further, we demonstrate that Lipschitz recurrent units are more robust with respect to perturbations. We evaluate our approach on a range of benchmark tasks, and we show it outperforms existing recurrent units.\",\n",
       "  'arxivId': '2006.12070',\n",
       "  'title': 'Lipschitz Recurrent Neural Networks',\n",
       "  'numCitations': 0}]"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "g.build_citations_subtree('1902.09689')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Paper Title: AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks \n",
       " \n",
       " 1) Abstract: \n",
       " Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: B. Chang\n",
       " \tURL: https://www.semanticscholar.org/author/144757437\n",
       " \n",
       " \t2) Name: Minmin Chen\n",
       " \tURL: https://www.semanticscholar.org/author/1743082\n",
       " \n",
       " \t3) Name: E. Haber\n",
       " \tURL: https://www.semanticscholar.org/author/145761835\n",
       " \n",
       " \t4) Name: Ed Huai-hsin Chi\n",
       " \tURL: https://www.semanticscholar.org/author/2226805\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/e2c8a6b49cd999b16ac4dcfdc375563a6932b1c7 \n",
       " \n",
       " 4) year: 2019 \n",
       " \n",
       " 5) fieldsOfStudy: ['Computer Science', 'Mathematics'] \n",
       " \n",
       " 6) numCitations: 7 \n",
       " \n",
       " 7) venue: ICLR \n",
       " \n",
       " 8) numReferences: 8 \n",
       " ]"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "g.citation_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'abstract': 'Physical phenomena in the real world are often described by energy-based modeling theories, such as Hamiltonian mechanics or the Landau theory, which yield various physical laws. Recent developments in neural networks have enabled the mimicking of the energy conservation law by learning the underlying continuous-time differential equations. However, this may not be possible in discrete time, which is often the case in practical learning and computation. Moreover, other physical laws have been overlooked in the previous neural network models. In this study, we propose a deep energy-based physical model that admits a specific differential geometric structure. From this structure, the conservation or dissipation law of energy and the mass conservation law follow naturally. To ensure the energetic behavior in discrete time, we also propose an automatic discrete differential algorithm that enables neural networks to employ the discrete gradient method.',\n",
       "  'arxivId': '1905.08604',\n",
       "  'title': 'Deep Energy-based Modeling of Discrete-Time Physics',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions. We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models, and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.',\n",
       "  'arxivId': '2001.03750',\n",
       "  'title': 'SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems',\n",
       "  'numCitations': 1},\n",
       " {'abstract': 'Time-reversal symmetry, which requires that the dynamics of a system should not change with the reversal of time axis, is a fundamental property that frequently holds in classical and quantum mechanics. In this paper, we propose a novel loss function that measures how well our ordinary differential equation (ODE) networks comply with this time-reversal symmetry; it is formally defined by the discrepancy in the time evolutions of ODE networks between forward and backward dynamics. Then, we design a new framework, which we name as Time-Reversal Symmetric ODE Networks (TRS-ODENs), that can learn the dynamics of physical systems more sample-efficiently by learning with the proposed loss function. We evaluate TRS-ODENs on several classical dynamics, and find they can learn the desired time evolution from observed noisy and complex trajectories. We also show that, even for systems that do not possess the full time-reversal symmetry, TRS-ODENs can achieve better predictive performances over baselines.',\n",
       "  'arxivId': '2007.11362',\n",
       "  'title': 'Time-Reversal Symmetric ODE Network',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we present a comparative analysis of the energyconserving neural networks for example, deep Lagrangian network, Hamiltonian neural network, etc. wherein the underlying physics is encoded in their computation graph. We focus on ten neural network models and explain the similarities and differences between the models. We compare their performance in 4 different physical systems. Our result highlights that using a high-dimensional coordinate system and then imposing restrictions via explicit constraints can lead to higher accuracy in the learned dynamics. We also point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.',\n",
       "  'arxivId': '2012.02334',\n",
       "  'title': 'Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works improve generalization for predicting trajectories by learning the Hamiltonian or Lagrangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the constraints explicitly with Lagrange multipliers dramatically simplifies the learning problem. We introduce a series of challenging chaotic and extended-body systems, including systems with N-pendulums, spring coupling, magnetic fields, rigid rotors, and gyroscopes, to push the limits of current approaches. Our experiments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efficiency.',\n",
       "  'arxivId': '2010.13581',\n",
       "  'title': 'Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit Constraints',\n",
       "  'numCitations': 1},\n",
       " {'abstract': 'Recurrent neural networks are widely used on time series data, yet such models often ignore the underlying physical structures in such sequences. A new class of physics-based methods related to Koopman theory has been introduced, offering an alternative for processing nonlinear dynamical systems. In this work, we propose a novel Consistent Koopman Autoencoder model which, unlike the majority of existing work, leverages the forward and backward dynamics. Key to our approach is a new analysis which explores the interplay between consistent dynamics and their associated Koopman operators. Our network is directly related to the derived analysis, and its computational requirements are comparable to other baselines. We evaluate our method on a wide range of high-dimensional and short-term dependent problems, and it achieves accurate estimates for significant prediction horizons, while also being robust to noise.',\n",
       "  'arxivId': '2003.02236',\n",
       "  'title': 'Forecasting Sequential Data using Consistent Koopman Autoencoders',\n",
       "  'numCitations': 1}]"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "g.build_citations_subtree('1909.13334')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Paper Title: AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks \n",
       " \n",
       " 1) Abstract: \n",
       " Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: B. Chang\n",
       " \tURL: https://www.semanticscholar.org/author/144757437\n",
       " \n",
       " \t2) Name: Minmin Chen\n",
       " \tURL: https://www.semanticscholar.org/author/1743082\n",
       " \n",
       " \t3) Name: E. Haber\n",
       " \tURL: https://www.semanticscholar.org/author/145761835\n",
       " \n",
       " \t4) Name: Ed Huai-hsin Chi\n",
       " \tURL: https://www.semanticscholar.org/author/2226805\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/e2c8a6b49cd999b16ac4dcfdc375563a6932b1c7 \n",
       " \n",
       " 4) year: 2019 \n",
       " \n",
       " 5) fieldsOfStudy: ['Computer Science', 'Mathematics'] \n",
       " \n",
       " 6) numCitations: 7 \n",
       " \n",
       " 7) venue: ICLR \n",
       " \n",
       " 8) numReferences: 8 \n",
       " ,\n",
       " Paper Title: Symplectic Recurrent Neural Networks \n",
       " \n",
       " 1) Abstract: \n",
       " We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: Zhengdao Chen\n",
       " \tURL: https://www.semanticscholar.org/author/8157979\n",
       " \n",
       " \t2) Name: J. Zhang\n",
       " \tURL: https://www.semanticscholar.org/author/49049937\n",
       " \n",
       " \t3) Name: Martín Arjovsky\n",
       " \tURL: https://www.semanticscholar.org/author/2877311\n",
       " \n",
       " \t4) Name: L. Bottou\n",
       " \tURL: https://www.semanticscholar.org/author/119267979\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/60ea57281e62fbf28e2022f085b99a9e01c06162 \n",
       " \n",
       " 4) year: 2020 \n",
       " \n",
       " 5) fieldsOfStudy: ['Computer Science', 'Mathematics'] \n",
       " \n",
       " 6) numCitations: 6 \n",
       " \n",
       " 7) venue: ICLR \n",
       " \n",
       " 8) numReferences: 3 \n",
       " ]"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "g.citation_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = paper.get_top_k_references_metadata(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "__main__.ArXivPaper"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "type(references[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = GraphNode(paper=paper, num_citations=20, num_references=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = node.get_reference_children_metadata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'abstract': 'Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings.',\n",
       "  'arxivId': '1708.00065',\n",
       "  'title': 'Time-Dependent Representation for Neural Event Sequence Prediction',\n",
       "  'numReferences': 5},\n",
       " {'abstract': 'Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.',\n",
       "  'arxivId': '1511.05942',\n",
       "  'title': 'Doctor AI: Predicting Clinical Events via Recurrent Neural Networks',\n",
       "  'numReferences': 2}]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cit = node.get_citation_children_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'abstract': 'Over the past few years, deep learning has risen to the foreground as a topic of massive interest, mainly as a result of successes obtained in solving large-scale image processing tasks. There are multiple challenging mathematical problems involved in applying deep learning: most deep learning methods require the solution of hard optimisation problems, and a good understanding of the tradeoff between computational effort, amount of data and model complexity is required to successfully design a deep learning approach for a given problem. A large amount of progress made in deep learning has been based on heuristic explorations, but there is a growing effort to mathematically understand the structure in existing deep learning methods and to systematically design new deep learning methods to preserve certain types of structure in deep learning. In this article, we review a number of these directions: some deep neural networks can be understood as discretisations of dynamical systems, neural networks can be designed to have desirable properties such as invertibility or group equivariance, and new algorithmic frameworks based on conformal Hamiltonian systems and Riemannian manifolds to solve the optimisation problems have been proposed. We conclude our review of each of these topics by discussing some open problems that we consider to be interesting directions for future research.',\n",
       "  'arxivId': '2006.03364',\n",
       "  'title': 'Structure preserving deep learning',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler.',\n",
       "  'arxivId': '1902.09689',\n",
       "  'title': 'AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks',\n",
       "  'numCitations': 7},\n",
       " {'abstract': 'We use neural ordinary differential equations to formulate a variant of the Transformer that is depth-adaptive in the sense that an input-dependent number of time steps is taken by the ordinary differential equation solver. Our goal in proposing the N-ODE Transformer is to investigate whether its depth-adaptivity may aid in overcoming some specific known theoretical limitations of the Transformer in handling nonlocal effects. Specifically, we consider the simple problem of determining the parity of a binary sequence, for which the standard Transformer has known limitations that can only be overcome by using a sufficiently large number of layers or attention heads. We find, however, that the depth-adaptivity of the N-ODE Transformer does not provide a remedy for the inherently nonlocal nature of the parity problem, and provide explanations for why this is so. Next, we pursue regularization of the N-ODE Transformer by penalizing the arclength of the ODE trajectories, but find that this fails to improve the accuracy or efficiency of the N-ODE Transformer on the challenging parity problem. We suggest future avenues of research for modifications and extensions of the N-ODE Transformer that may lead to improved accuracy and efficiency for sequence modelling tasks such as neural machine translation.',\n",
       "  'arxivId': '2010.11358',\n",
       "  'title': 'N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using Neural Ordinary Differential Equations',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of \\\\emph{controlled differential equations}. The resulting \\\\emph{neural controlled differential equation} model is directly applicable to the general setting of partially-observed irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efficient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models.',\n",
       "  'arxivId': '2005.08926',\n",
       "  'title': 'Neural Controlled Differential Equations for Irregular Time Series',\n",
       "  'numCitations': 2},\n",
       " {'abstract': \"Normalizing flows attempt to model an arbitrary probability distribution through a set of invertible mappings. These transformations are required to achieve a tractable Jacobian determinant that can be used in high-dimensional scenarios. The first normalizing flow designs used coupling layer mappings built upon affine transformations. The significant advantage of such models is their easy-to-compute inverse. Nevertheless, making use of affine transformations may limit the expressiveness of such models. Recently, invertible piecewise polynomial functions as a replacement for affine transformations have attracted attention. However, these methods require solving a polynomial equation to calculate their inverse. In this paper, we explore using linear rational splines as a replacement for affine transformations used in coupling layers. Besides having a straightforward inverse, inference and generation have similar cost and architecture in this method. Moreover, simulation results demonstrate the competitiveness of this approach's performance compared to existing methods.\",\n",
       "  'arxivId': '2001.05168',\n",
       "  'title': 'Invertible Generative Modeling using Linear Rational Splines',\n",
       "  'numCitations': 1},\n",
       " {'abstract': 'Recent work has attempted to interpret residual networks (ResNets) as one step of a forward Euler discretization of an ordinary differential equation, focusing mainly on syntactic algebraic similarities between the two systems. Discrete dynamical integrators of continuous dynamical systems, however, have a much richer structure. We first show that ResNets fail to be meaningful dynamical integrators in this richer sense. We then demonstrate that neural network models can learn to represent continuous dynamical systems, with this richer structure and properties, by embedding them into higher-order numerical integration schemes, such as the Runge Kutta schemes. Based on these insights, we introduce ContinuousNet as a continuous-in-depth generalization of ResNet architectures. ContinuousNets exhibit an invariance to the particular computational graph manifestation. That is, the continuous-in-depth model can be evaluated with different discrete time step sizes, which changes the number of layers, and different numerical integration schemes, which changes the graph connectivity. We show that this can be used to develop an incremental-in-depth training scheme that improves model quality, while significantly decreasing training time. We also show that, once trained, the number of units in the computational graph can even be decreased, for faster inference with little-to-no accuracy drop.',\n",
       "  'arxivId': '2008.02389',\n",
       "  'title': 'Continuous-in-Depth Neural Networks',\n",
       "  'numCitations': 0},\n",
       " {'abstract': \"A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE's dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow's Jacobian can be obtained by integrating the trace of the dynamics' Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to a state-of-the-art CNF while on average requiring one-fourth of the number of weights with 19x speedup in training time and 28x speedup in inference.\",\n",
       "  'arxivId': '2006.00104',\n",
       "  'title': 'OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport',\n",
       "  'numCitations': 1},\n",
       " {'abstract': 'Predicting future states or actions of a given system remains a fundamental, yet unsolved challenge of intelligence, especially in the scope of complex and nondeterministic scenarios, such as modeling behavior of humans. Existing approaches provide results under strong assumptions concerning unimodality of future states, or, at best, assuming specific probability distributions that often poorly fit to real-life conditions. In this work we introduce a robust and flexible probabilistic framework that allows to model future predictions with virtually no constrains regarding the modality or underlying probability distribution. To achieve this goal, we leverage a hypernetwork architecture and train a continuous normalizing flow model. The resulting method dubbed RegFlow achieves state-of-the-art results on several benchmark datasets, outperforming competing approaches by a significant margin.',\n",
       "  'arxivId': '2011.14620',\n",
       "  'title': 'RegFlow: Probabilistic Flow-based Regression for Future Prediction',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Fueled by the availability of more data and computing power, recent breakthroughs in cloud-based machine learning (ML) have transformed every aspect of our lives from face recognition and medical diagnosis to natural language processing. However, classical ML exerts severe demands in terms of energy, memory, and computing resources, limiting their adoption for resource-constrained edge devices. The new breed of intelligent devices and high-stake applications (drones, augmented/virtual reality, autonomous systems, and so on) requires a novel paradigm change calling for distributed, low-latency and reliable ML at the wireless network edge (referred to as edge ML). In edge ML, training data are unevenly distributed over a large number of edge nodes, which have access to a tiny fraction of the data. Moreover, training and inference are carried out collectively over wireless links, where edge devices communicate and exchange their learned models (not their private data). In a first of its kind, this article explores the key building blocks of edge ML, different neural network architectural splits and their inherent tradeoffs, as well as theoretical and technical enablers stemming from a wide range of mathematical disciplines. Finally, several case studies pertaining to various high-stake applications are presented to demonstrate the effectiveness of edge ML in unlocking the full potential of 5G and beyond.',\n",
       "  'arxivId': '1812.02858',\n",
       "  'title': 'Wireless Network Intelligence at the Edge',\n",
       "  'numCitations': 3},\n",
       " {'abstract': 'Model noise is known to have detrimental effects on neural networks, such as training instability and predictive distributions with non-calibrated uncertainty properties. These factors set bottlenecks on the expressive potential of Neural Stochastic Differential Equations (NSDEs), a model family that employs neural nets on both drift and diffusion functions. We introduce a novel algorithm that solves a generic NSDE using only deterministic approximation methods. Given a discretization, we estimate the marginal distribution of the Ito process implied by the NSDE using a recursive scheme to propagate deterministic approximations of the statistical moments across time steps. The proposed algorithm comes with theoretical guarantees on numerical stability and convergence to the true solution, enabling its computational use for robust, accurate, and efficient prediction of long sequences. We observe our novel algorithm to behave interpretably on synthetic setups and to improve the state of the art on two challenging real-world tasks.',\n",
       "  'arxivId': '2006.08973',\n",
       "  'title': 'Deterministic Inference of Neural Stochastic Differential Equations',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Neural differential equations are a promising new member in the neural network family. They show the potential of differential equations for time-series data analysis. In this paper, the strength of the ordinary differential equation (ODE) is explored with a new extension. The main goal of this work is to answer the following questions: (i) can ODE be used to redefine the existing neural network model? (ii) can Neural ODEs solve the irregular sampling rate challenge of existing neural network models for a continuous time series, i.e., length and dynamic nature, (iii) how to reduce the training and evaluation time of existing Neural ODE systems? This work leverages the mathematical foundation of ODEs to redesign traditional RNNs such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The main contribution of this paper is to illustrate the design of two new ODE-based RNN models (GRU-ODE model and LSTM-ODE) which can compute the hidden state and cell state at any point of time using an ODE solver. These models reduce the computation overhead of hidden state and cell state by a vast amount. The performance evaluation of these two new models for learning continuous time series with irregular sampling rate is then demonstrated. Experiments show that these new ODE based RNN models require less training time than Latent ODEs and conventional Neural ODEs. They can achieve higher accuracy quickly, and the design of the neural network is more straightforward than the previous neural ODE systems.',\n",
       "  'arxivId': '2005.09807',\n",
       "  'title': 'Neural Ordinary Differential Equation based Recurrent Neural Network Model',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Compressing deep neural networks (DNNs) is important for real-world applications operating on resource-constrained devices. However, it is not straightforward to change the model size (i.e., computational complexity) once training and compression are completed, calling for retraining to construct models suitable for different devices. In this paper, we propose a novel method, Decomposable-Net (the network decomposable in any size), which allows flexible changes to model size without retraining. We decompose weight matrices in the DNNs via singular value decomposition and adjust ranks according to the target model size. Unlike the existing methods, (1) we propose a learning method that explicitly minimizes losses for both of full-rank and low-rank networks, which is designed not only to maintain the performance of a full-rank network but also to improve multiple low-rank networks in a single model. (2) We also provide a mathematical analysis for the scalability of the approximation error with respect to the rank in each layer. Moreover, on the basis of the analysis, (3) we introduce a simple criterion for rank selection that effectively suppresses approximation error. In experiments on image-classification tasks on CIFAR-10/100 and ImageNet datasets, Decomposable-Net yields favorable performance in a broader range of compressed models. In particular, Decomposable-Net achieves the top-1 accuracy of $73.2\\\\%$ with $0.27\\\\times$MACs on the ImageNet classification task with ResNet-50, compared to low-rank tensor (Tucker) decomposition ($67.4\\\\% / 0.30\\\\times$) and universally slimmable networks ($70.6\\\\% / 0.26\\\\times$).',\n",
       "  'arxivId': '1910.13141',\n",
       "  'title': 'Decomposable-Net: Scalable Low-Rank Compression for Neural Networks',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'We introduce SchrodingeRNN, a quantum inspired generative model for raw audio. Audio data is wave-like and is sampled from a continuous signal. Although generative modelling of raw audio has made great strides lately, relational inductive biases relevant to these two characteristics are mostly absent from models explored to date. Quantum Mechanics is a natural source of probabilistic models of wave behaviour. Our model takes the form of a stochastic Schrodinger equation describing the continuous time measurement of a quantum system, and is equivalent to the continuous Matrix Product State (cMPS) representation of wavefunctions in one dimensional many-body systems. This constitutes a deep autoregressive architecture in which the systems state is a latent representation of the past observations. We test our model on synthetic data sets of stationary and non-stationary signals. This is the first time cMPS are used in machine learning.',\n",
       "  'arxivId': '1911.11879',\n",
       "  'title': 'SchrödingeRNN: Generative Modeling of Raw Audio as a Continuously Observed Quantum State',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Stochastic regularization of neural networks (e.g. dropout) is a wide-spread technique in deep learning that allows for better generalization. Despite its success, continuous-time models, such as neural ordinary differential equation (ODE), usually rely on a completely deterministic feed-forward operation. This work provides an empirical study of stochastically regularized neural ODE on several image-classification tasks (CIFAR-10, CIFAR-100, TinyImageNet). Building upon the formalism of stochastic differential equations (SDEs), we demonstrate that neural SDE is able to outperform its deterministic counterpart. Further, we show that data augmentation during the training improves the performance of both deterministic and stochastic versions of the same model. However, the improvements obtained by the data augmentation completely eliminate the empirical gains of the stochastic regularization, making the difference in the performance of neural ODE and neural SDE negligible.',\n",
       "  'arxivId': '2002.09779',\n",
       "  'title': 'Stochasticity in Neural ODEs: An Empirical Study',\n",
       "  'numCitations': 1},\n",
       " {'abstract': 'Normalizing flows are a powerful technique for obtaining reparameterizable samples from complex multimodal distributions. Unfortunately current approaches fall short when the underlying space has a non trivial topology, and are only available for the most basic geometries. Recently normalizing flows in Euclidean space based on Neural ODEs show great promise, yet suffer the same limitations. Using ideas from differential geometry and geometric control theory, we describe how neural ODEs can be extended to smooth manifolds. We show how vector fields provide a general framework for parameterizing a flexible class of invertible mapping on these spaces and we illustrate how gradient based learning can be performed. As a result we define a general methodology for building normalizing flows on manifolds.',\n",
       "  'arxivId': '2006.06663',\n",
       "  'title': 'Neural Ordinary Differential Equations on Manifolds',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters.',\n",
       "  'arxivId': '2010.04456',\n",
       "  'title': 'Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'This paper proposes the use of spectral element methods \\\\citep{canuto_spectral_1988} for fast and accurate training of Neural Ordinary Differential Equations (ODE-Nets; \\\\citealp{Chen2018NeuralOD}) for system identification. This is achieved by expressing their dynamics as a truncated series of Legendre polynomials. The series coefficients, as well as the network weights, are computed by minimizing the weighted sum of the loss function and the violation of the ODE-Net dynamics. The problem is solved by coordinate descent that alternately minimizes, with respect to the coefficients and the weights, two unconstrained sub-problems using standard backpropagation and gradient methods. The resulting optimization scheme is fully time-parallel and results in a low memory footprint. Experimental comparison to standard methods, such as backpropagation through explicit solvers and the adjoint technique \\\\citep{Chen2018NeuralOD}, on training surrogate models of small and medium-scale dynamical systems shows that it is at least one order of magnitude faster at reaching a comparable value of the loss function. The corresponding testing MSE is one order of magnitude smaller as well, suggesting generalization capabilities increase.',\n",
       "  'arxivId': '1906.07038',\n",
       "  'title': 'SNODE: Spectral Discretization of Neural ODEs for System Identification',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'We introduce a provably stable variant of neural ordinary differential equations (neural ODEs) whose trajectories evolve on an energy functional parametrised by a neural network. Stable neural flows provide an implicit guarantee on asymptotic stability of the depth-flows, leading to robustness against input perturbations and low computational burden for the numerical solver. The learning procedure is cast as an optimal control problem, and an approximate solution is proposed based on adjoint sensivity analysis. We further introduce novel regularizers designed to ease the optimization process and speed up convergence. The proposed model class is evaluated on non-linear classification and function approximation tasks.',\n",
       "  'arxivId': '2003.08063',\n",
       "  'title': 'Stable Neural Flows',\n",
       "  'numCitations': 0},\n",
       " {'abstract': 'Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe Zygote, a Differentiable Programming system that is able to take gradients of general program structures. We implement this system in the Julia programming language. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning, but more importantly, it enables us to incorporate a large ecosystem of libraries in our models in a straightforward way. We discuss our approach to automatic differentiation, including its support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present several examples of differentiating programs.',\n",
       "  'arxivId': '1907.07587',\n",
       "  'title': 'A Differentiable Programming System to Bridge Machine Learning and Scientific Computing',\n",
       "  'numCitations': 2},\n",
       " {'abstract': 'A convolutional neural network for image classification can be constructed following some mathematical ways since it models the ventral stream in visual cortex which is regarded as a multi-period dynamical system. In this paper, a new point of view is proposed for constructing network models as well as providing a direction to get inspiration or explanation for neural network. If each period in ventral stream was deemed to be a dynamical system with time as the independent variable, there should be a set of ordinary differential equations (ODEs) for this system. Runge-Kutta methods are common means to solve ODE. Thus, network model ought to be built using these methods. Moreover, convolutional networks could be employed to emulate the increments within every time-step. The model constructed in the above way is named Runge-Kutta Convolutional Neural Network (RKNet). According to this idea, Dense Convolutional Networks (DenseNets) and Residual Networks (ResNets) were varied to RKNets. To prove the feasibility of RKNets, these variants were verified on benchmark datasets, CIFAR and ImageNet. The experimental results show that the RKNets transformed from DenseNets gained similar or even higher parameter efficiency. The success of the experiments denotes that Runge-Kutta methods can be utilized to construct convolutional neural networks for image classification efficiently. Furthermore, the network models might be structured more rationally in the future basing on RKNet and priori knowledge.',\n",
       "  'arxivId': '1802.08831',\n",
       "  'title': 'Convolutional Neural Networks combined with Runge-Kutta Methods',\n",
       "  'numCitations': 1}]"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.get_reference_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'arxivId': '2006.03364',\n",
       "  'authors': [{'authorId': '2791391', 'name': 'E. Celledoni'},\n",
       "   {'authorId': '48125860', 'name': 'Matthias Joachim Ehrhardt'},\n",
       "   {'authorId': '11371884', 'name': 'Christian Etmann'},\n",
       "   {'authorId': '47283982', 'name': 'R. McLachlan'},\n",
       "   {'authorId': '1868160', 'name': 'B. Owren'},\n",
       "   {'authorId': '1711104', 'name': 'C. Schönlieb'},\n",
       "   {'authorId': '52585009', 'name': 'F. Sherry'}],\n",
       "  'title': 'Structure preserving deep learning',\n",
       "  'url': 'https://www.semanticscholar.org/paper/96efa5af47c75fe90909cbceafe7524714c9e5b9',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '1902.09689',\n",
       "  'authors': [{'authorId': '144757437', 'name': 'B. Chang'},\n",
       "   {'authorId': '1743082', 'name': 'Minmin Chen'},\n",
       "   {'authorId': '145761835', 'name': 'E. Haber'},\n",
       "   {'authorId': '2226805', 'name': 'Ed Huai-hsin Chi'}],\n",
       "  'title': 'AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks',\n",
       "  'url': 'https://www.semanticscholar.org/paper/e2c8a6b49cd999b16ac4dcfdc375563a6932b1c7',\n",
       "  'venue': 'ICLR',\n",
       "  'year': 2019},\n",
       " {'arxivId': '2010.11358',\n",
       "  'authors': [{'authorId': '2000567397', 'name': 'Aaron Baier-Reinio'},\n",
       "   {'authorId': '8326935', 'name': 'H. D. Sterck'}],\n",
       "  'title': 'N-ODE Transformer: A Depth-Adaptive Variant of the Transformer Using Neural Ordinary Differential Equations',\n",
       "  'url': 'https://www.semanticscholar.org/paper/54ea76f4e7099532874924ef76d0cd6f3161bbe7',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2005.08926',\n",
       "  'authors': [{'authorId': '120149052', 'name': 'Patrick Kidger'},\n",
       "   {'authorId': '77282289', 'name': 'James Morrill'},\n",
       "   {'authorId': '39671663', 'name': 'J. Foster'},\n",
       "   {'authorId': '144749402', 'name': 'Terry Lyons'}],\n",
       "  'title': 'Neural Controlled Differential Equations for Irregular Time Series',\n",
       "  'url': 'https://www.semanticscholar.org/paper/5419ec6f2880b0e664c21fb96fac2f210bbfc5fa',\n",
       "  'venue': 'NeurIPS',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2001.05168',\n",
       "  'authors': [{'authorId': '123772869', 'name': 'Hadi M. Dolatabadi'},\n",
       "   {'authorId': '144757691', 'name': 'S. Erfani'},\n",
       "   {'authorId': '1688394', 'name': 'C. Leckie'}],\n",
       "  'title': 'Invertible Generative Modeling using Linear Rational Splines',\n",
       "  'url': 'https://www.semanticscholar.org/paper/ca03a49ba4e06f85f64bf22f3110d534cb58e02a',\n",
       "  'venue': 'AISTATS',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2008.02389',\n",
       "  'authors': [{'authorId': '40897456', 'name': 'A. Queiruga'},\n",
       "   {'authorId': '2371914', 'name': 'N. Erichson'},\n",
       "   {'authorId': '34174929', 'name': 'D. Taylor'},\n",
       "   {'authorId': '1717098', 'name': 'M. W. Mahoney'}],\n",
       "  'title': 'Continuous-in-Depth Neural Networks',\n",
       "  'url': 'https://www.semanticscholar.org/paper/f612193a890844c5ecb4605e6a87000957b6487c',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2006.00104',\n",
       "  'authors': [{'authorId': '31509385', 'name': 'Derek Onken'},\n",
       "   {'authorId': '143745940', 'name': 'Samy Wu Fung'},\n",
       "   {'authorId': '7824051', 'name': 'Xingjian Li'},\n",
       "   {'authorId': '49418655', 'name': 'Lars Ruthotto'}],\n",
       "  'title': 'OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport',\n",
       "  'url': 'https://www.semanticscholar.org/paper/c837226031bde30ae4b886ba8061eee979083dbc',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2011.14620',\n",
       "  'authors': [{'authorId': '3027512', 'name': 'M. Zieba'},\n",
       "   {'authorId': '147572870', 'name': 'Marcin Przewiezlikowski'},\n",
       "   {'authorId': '49688210', 'name': 'M. Smieja'},\n",
       "   {'authorId': '145541197', 'name': 'J. Tabor'},\n",
       "   {'authorId': '144432036', 'name': 'T. Trzciński'},\n",
       "   {'authorId': '153673689', 'name': 'Przemysław Spurek'}],\n",
       "  'title': 'RegFlow: Probabilistic Flow-based Regression for Future Prediction',\n",
       "  'url': 'https://www.semanticscholar.org/paper/4836cad4b69b09e6002ddb1921df173d31c91dcb',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '1812.02858',\n",
       "  'authors': [{'authorId': '48490823', 'name': 'Jihong Park'},\n",
       "   {'authorId': '2293043', 'name': 'S. Samarakoon'},\n",
       "   {'authorId': '1702172', 'name': 'M. Bennis'},\n",
       "   {'authorId': '145118318', 'name': 'M. Debbah'}],\n",
       "  'title': 'Wireless Network Intelligence at the Edge',\n",
       "  'url': 'https://www.semanticscholar.org/paper/7f7bb204806ed819b323953cf6ca04cc65a27698',\n",
       "  'venue': 'Proceedings of the IEEE',\n",
       "  'year': 2019},\n",
       " {'arxivId': '2006.08973',\n",
       "  'authors': [{'authorId': '1395309619', 'name': 'Andreas Look'},\n",
       "   {'authorId': '48755342', 'name': 'Chen Qiu'},\n",
       "   {'authorId': '152889403', 'name': 'M. Rudolph'},\n",
       "   {'authorId': '144719340', 'name': 'J. Peters'},\n",
       "   {'authorId': '11213556', 'name': 'M. Kandemir'}],\n",
       "  'title': 'Deterministic Inference of Neural Stochastic Differential Equations',\n",
       "  'url': 'https://www.semanticscholar.org/paper/9096d742369fc21ce6d84275b3b201ab25efc519',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2005.09807',\n",
       "  'authors': [{'authorId': '35238485', 'name': 'M. Habiba'},\n",
       "   {'authorId': '1700974', 'name': 'Barak A. Pearlmutter'}],\n",
       "  'title': 'Neural Ordinary Differential Equation based Recurrent Neural Network Model',\n",
       "  'url': 'https://www.semanticscholar.org/paper/ced5650cc85e8e7c569f65a0163f3b178cc96108',\n",
       "  'venue': '2020 31st Irish Signals and Systems Conference (ISSC)',\n",
       "  'year': 2020},\n",
       " {'arxivId': '1910.13141',\n",
       "  'authors': [{'authorId': '2579842', 'name': 'A. Yaguchi'},\n",
       "   {'authorId': '40617873', 'name': 'T. Suzuki'},\n",
       "   {'authorId': '2837282', 'name': 'Shuhei Nitta'},\n",
       "   {'authorId': '144112928', 'name': 'Y. Sakata'},\n",
       "   {'authorId': '2184803', 'name': 'A. Tanizawa'}],\n",
       "  'title': 'Scalable Deep Neural Networks via Low-Rank Matrix Factorization',\n",
       "  'url': 'https://www.semanticscholar.org/paper/ac6cf8338ae411c2a159868d327073480a33d0d3',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2019},\n",
       " {'arxivId': '1911.11879',\n",
       "  'authors': [{'authorId': '1432236228', 'name': 'Beñat Mencia Uranga'},\n",
       "   {'authorId': '7702763', 'name': 'A. Lamacraft'}],\n",
       "  'title': 'SchrödingeRNN: Generative Modeling of Raw Audio as a Continuously Observed Quantum State',\n",
       "  'url': 'https://www.semanticscholar.org/paper/1d6abb40f49b9ff13ab58cad014088c6fff48ca5',\n",
       "  'venue': 'MSML',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2002.09779',\n",
       "  'authors': [{'authorId': '144110808', 'name': 'V. Oganesyan'},\n",
       "   {'authorId': '88901018', 'name': 'A. Volokhova'},\n",
       "   {'authorId': '2492721', 'name': 'D. Vetrov'}],\n",
       "  'title': 'Stochasticity in Neural ODEs: An Empirical Study',\n",
       "  'url': 'https://www.semanticscholar.org/paper/c2df715a1968a8884a05f45f4b865da329ff5dda',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2006.06663',\n",
       "  'authors': [{'authorId': '41020959', 'name': 'Luca Falorsi'},\n",
       "   {'authorId': '51131843', 'name': 'Patrick Forré'}],\n",
       "  'title': 'Neural Ordinary Differential Equations on Manifolds',\n",
       "  'url': 'https://www.semanticscholar.org/paper/885ab11496e15baf507fbad85dba767e2a82bb21',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2010.04456',\n",
       "  'authors': [{'authorId': '3965182', 'name': 'V. Guen'},\n",
       "   {'authorId': '1725051', 'name': 'Yuan Yin'},\n",
       "   {'authorId': '1853488882', 'name': \"J'er'emie Dona\"},\n",
       "   {'authorId': '10771473', 'name': 'I. Ayed'},\n",
       "   {'authorId': '29961943', 'name': 'Emmanuel de Bézenac'},\n",
       "   {'authorId': '1728523', 'name': 'N. Thome'},\n",
       "   {'authorId': '67192547', 'name': 'P. Gallinari'}],\n",
       "  'title': 'Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting',\n",
       "  'url': 'https://www.semanticscholar.org/paper/2950199278e33df3b91ca01a366552b9b3f806fc',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '1906.07038',\n",
       "  'authors': [{'authorId': '49086703', 'name': 'A. Quaglino'},\n",
       "   {'authorId': '2066311', 'name': 'Marco Gallieri'},\n",
       "   {'authorId': '2426718', 'name': 'J. Masci'},\n",
       "   {'authorId': '2865775', 'name': 'J. Koutník'}],\n",
       "  'title': 'SNODE: Spectral Discretization of Neural ODEs for System Identification',\n",
       "  'url': 'https://www.semanticscholar.org/paper/6f29b447780f166e6cf4532669aa1d86803a30b5',\n",
       "  'venue': 'ICLR',\n",
       "  'year': 2020},\n",
       " {'arxivId': '2003.08063',\n",
       "  'authors': [{'authorId': '90467999', 'name': 'Stefano Massaroli'},\n",
       "   {'authorId': '40585370', 'name': 'Michael Poli'},\n",
       "   {'authorId': '8420701', 'name': 'Michelangelo Bin'},\n",
       "   {'authorId': '48490286', 'name': 'Jinkyoo Park'},\n",
       "   {'authorId': '152521159', 'name': 'A. Yamashita'},\n",
       "   {'authorId': '50631807', 'name': 'H. Asama'}],\n",
       "  'title': 'Stable Neural Flows',\n",
       "  'url': 'https://www.semanticscholar.org/paper/6c67ac2a6a79c29620981dba521ead930e562fd8',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2020},\n",
       " {'arxivId': '1907.07587',\n",
       "  'authors': [{'authorId': '34289387', 'name': 'Mike Innes'},\n",
       "   {'authorId': '144397527', 'name': 'A. Edelman'},\n",
       "   {'authorId': '47247177', 'name': 'K. Fischer'},\n",
       "   {'authorId': '150956309', 'name': 'C. Rackauckas'},\n",
       "   {'authorId': '2377846', 'name': 'E. Saba'},\n",
       "   {'authorId': '2543935', 'name': 'V. B. Shah'},\n",
       "   {'authorId': '90684986', 'name': 'Will Tebbutt'}],\n",
       "  'title': 'A Differentiable Programming System to Bridge Machine Learning and Scientific Computing',\n",
       "  'url': 'https://www.semanticscholar.org/paper/b208cff55a44f9daa82972f81690be29a2677e2d',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2019},\n",
       " {'arxivId': '1802.08831',\n",
       "  'authors': [{'authorId': '35761591', 'name': 'M. Zhu'},\n",
       "   {'authorId': '3259048', 'name': 'C. Fu'}],\n",
       "  'title': 'Convolutional Neural Networks combined with Runge-Kutta Methods',\n",
       "  'url': 'https://www.semanticscholar.org/paper/cda6d9b0cc1509ec61a43e24e9b7a14b18642aca',\n",
       "  'venue': 'ArXiv',\n",
       "  'year': 2018}]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "node.citation_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'arxivId': '1708.00065',\n",
       "  'authors': [{'authorId': None, 'name': 'Yang Li'},\n",
       "   {'authorId': '145585757', 'name': 'Nan Du'},\n",
       "   {'authorId': '1751569', 'name': 'S. Bengio'}],\n",
       "  'title': 'Time-Dependent Representation for Neural Event Sequence Prediction',\n",
       "  'url': 'https://www.semanticscholar.org/paper/ec7bab52b2220a6cad410dd82b3fbe140d2196f0',\n",
       "  'venue': 'ICLR',\n",
       "  'year': 2018},\n",
       " {'arxivId': '1606.04130',\n",
       "  'authors': [{'authorId': '32219137', 'name': 'Zachary Chase Lipton'},\n",
       "   {'authorId': '2107807', 'name': 'David C. Kale'},\n",
       "   {'authorId': '144616817', 'name': 'R. Wetzel'}],\n",
       "  'title': 'Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series',\n",
       "  'url': 'https://www.semanticscholar.org/paper/562f33611cdc0d8ed6609aa09f153e6238d5409e',\n",
       "  'venue': 'MLHC',\n",
       "  'year': 2016},\n",
       " {'arxivId': '1505.05770',\n",
       "  'authors': [{'authorId': '1748523', 'name': 'Danilo Jimenez Rezende'},\n",
       "   {'authorId': '14594344', 'name': 'S. Mohamed'}],\n",
       "  'title': 'Variational Inference with Normalizing Flows',\n",
       "  'url': 'https://www.semanticscholar.org/paper/0f899b92b7fb03b609fee887e4b6f3b633eaf30d',\n",
       "  'venue': 'ICML',\n",
       "  'year': 2015},\n",
       " {'arxivId': '1511.05942',\n",
       "  'authors': [{'authorId': '1387328701', 'name': 'Edward Choi'},\n",
       "   {'authorId': '2342604', 'name': 'M. T. Bahadori'},\n",
       "   {'authorId': '20079790', 'name': 'A. Schuetz'},\n",
       "   {'authorId': '143691397', 'name': 'W. Stewart'},\n",
       "   {'authorId': '49991208', 'name': 'Jimeng Sun'}],\n",
       "  'title': 'Doctor AI: Predicting Clinical Events via Recurrent Neural Networks',\n",
       "  'url': 'https://www.semanticscholar.org/paper/c434fce129f205a31871d51c0706b6aa9cfcb3b5',\n",
       "  'venue': 'MLHC',\n",
       "  'year': 2016}]"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "node.reference_childen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'arxivId': '1708.00065',\n",
       "  'authors': [{'authorId': None, 'name': 'Yang Li'},\n",
       "   {'authorId': '145585757', 'name': 'Nan Du'},\n",
       "   {'authorId': '1751569', 'name': 'S. Bengio'}],\n",
       "  'title': 'Time-Dependent Representation for Neural Event Sequence Prediction',\n",
       "  'url': 'https://www.semanticscholar.org/paper/ec7bab52b2220a6cad410dd82b3fbe140d2196f0',\n",
       "  'venue': 'ICLR',\n",
       "  'year': 2018},\n",
       " {'arxivId': '1606.04130',\n",
       "  'authors': [{'authorId': '32219137', 'name': 'Zachary Chase Lipton'},\n",
       "   {'authorId': '2107807', 'name': 'David C. Kale'},\n",
       "   {'authorId': '144616817', 'name': 'R. Wetzel'}],\n",
       "  'title': 'Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series',\n",
       "  'url': 'https://www.semanticscholar.org/paper/562f33611cdc0d8ed6609aa09f153e6238d5409e',\n",
       "  'venue': 'MLHC',\n",
       "  'year': 2016},\n",
       " {'arxivId': '1505.05770',\n",
       "  'authors': [{'authorId': '1748523', 'name': 'Danilo Jimenez Rezende'},\n",
       "   {'authorId': '14594344', 'name': 'S. Mohamed'}],\n",
       "  'title': 'Variational Inference with Normalizing Flows',\n",
       "  'url': 'https://www.semanticscholar.org/paper/0f899b92b7fb03b609fee887e4b6f3b633eaf30d',\n",
       "  'venue': 'ICML',\n",
       "  'year': 2015}]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_papers = paper.get_top_k_references_metadata(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Paper Title: Time-Dependent Representation for Neural Event Sequence Prediction \n",
       " \n",
       " 1) Abstract: \n",
       " Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. In this work, we propose a set of methods for using time in sequence prediction. Because neural sequence models such as RNN are more amenable for handling token-like input, we propose two methods for time-dependent event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We also introduce two methods for using next event duration as regularization for training a sequence prediction model. We discuss these methods based on recurrent neural nets. We evaluate these methods as well as baseline models on five datasets that resemble a variety of sequence prediction tasks. The experiments revealed that the proposed methods offer accuracy gain over baseline models in a range of settings. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: Yang Li\n",
       " \tURL: None\n",
       " \n",
       " \t2) Name: Nan Du\n",
       " \tURL: https://www.semanticscholar.org/author/145585757\n",
       " \n",
       " \t3) Name: S. Bengio\n",
       " \tURL: https://www.semanticscholar.org/author/1751569\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/ec7bab52b2220a6cad410dd82b3fbe140d2196f0 \n",
       " \n",
       " 4) year: 2018 \n",
       " \n",
       " 5) fieldsOfStudy: ['Computer Science', 'Mathematics'] \n",
       " \n",
       " 6) numCitations: 28 \n",
       " \n",
       " 7) venue: ICLR \n",
       " \n",
       " 8) numReferences: 25 \n",
       " ,\n",
       " Paper Title: Directly Modeling Missing Data in Sequences with RNNs: Improved Classification of Clinical Time Series \n",
       " \n",
       " 1) Abstract: \n",
       " We demonstrate a simple strategy to cope with missing data in sequential inputs, addressing the task of multilabel classification of diagnoses given clinical time series. Collected from the intensive care unit (ICU) of a major urban medical center, our data consists of multivariate time series of observations. The data is irregularly sampled, leading to missingness patterns in re-sampled sequences. In this work, we show the remarkable ability of RNNs to make effective use of binary indicators to directly model missing data, improving AUC and F1 significantly. However, while RNNs can learn arbitrary functions of the missing data and observations, linear models can only learn substitution values. For linear models and MLPs, we show an alternative strategy to capture this signal. Additionally, we evaluate LSTMs, MLPs, and linear models trained on missingness patterns only, showing that for several diseases, what tests are run can be more predictive than the results themselves. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: Zachary Chase Lipton\n",
       " \tURL: https://www.semanticscholar.org/author/32219137\n",
       " \n",
       " \t2) Name: David C. Kale\n",
       " \tURL: https://www.semanticscholar.org/author/2107807\n",
       " \n",
       " \t3) Name: R. Wetzel\n",
       " \tURL: https://www.semanticscholar.org/author/144616817\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/562f33611cdc0d8ed6609aa09f153e6238d5409e \n",
       " \n",
       " 4) year: 2016 \n",
       " \n",
       " 5) fieldsOfStudy: ['Computer Science', 'Mathematics'] \n",
       " \n",
       " 6) numCitations: 108 \n",
       " \n",
       " 7) venue: MLHC \n",
       " \n",
       " 8) numReferences: 26 \n",
       " ,\n",
       " Paper Title: Variational Inference with Normalizing Flows \n",
       " \n",
       " 1) Abstract: \n",
       " The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference. \n",
       " \n",
       " 2) Authors:\n",
       " \t1) Name: Danilo Jimenez Rezende\n",
       " \tURL: https://www.semanticscholar.org/author/1748523\n",
       " \n",
       " \t2) Name: S. Mohamed\n",
       " \tURL: https://www.semanticscholar.org/author/14594344\n",
       " \n",
       " 3) url: https://www.semanticscholar.org/paper/0f899b92b7fb03b609fee887e4b6f3b633eaf30d \n",
       " \n",
       " 4) year: 2015 \n",
       " \n",
       " 5) fieldsOfStudy: ['Mathematics', 'Computer Science'] \n",
       " \n",
       " 6) numCitations: 1225 \n",
       " \n",
       " 7) venue: ICML \n",
       " \n",
       " 8) numReferences: 40 \n",
       " ]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "references_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in paper['citations']:\n",
    "    if i['isInfluential'] == True:\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'arxivId': None,\n",
       " 'authors': [{'authorId': '104314859', 'name': 'Fred Daum'},\n",
       "  {'authorId': '50535618', 'name': 'J. Huang'},\n",
       "  {'authorId': '9130376', 'name': 'A. Noushin'}],\n",
       " 'doi': '10.1117/12.2517980',\n",
       " 'intent': ['background'],\n",
       " 'isInfluential': False,\n",
       " 'paperId': 'd13739de9b7e22eea9ff03c23d322817c14bdfd8',\n",
       " 'title': \"Extremely deep Bayesian learning with Gromov's method\",\n",
       " 'url': 'https://www.semanticscholar.org/paper/d13739de9b7e22eea9ff03c23d322817c14bdfd8',\n",
       " 'venue': 'Defense + Commercial Sensing',\n",
       " 'year': 2019}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "paper.paper['citations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}