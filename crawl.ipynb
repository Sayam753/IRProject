{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import warnings\n",
    "\n",
    "class SemanticScholarMetaDataExtractor():\n",
    "    def __init__(self):\n",
    "        self.API_ID = 'https://api.semanticscholar.org/v1/paper/arXiv:'\n",
    "\n",
    "    def get_response(self, paper_id):\n",
    "        paper_url = self.API_ID+paper_id\n",
    "        return urllib.request.urlopen(paper_url)\n",
    "    \n",
    "    def get_data_json(self, paper_id):\n",
    "        response = self.get_response(paper_id)\n",
    "        return json.loads(response.read())\n",
    "\n",
    "class ArXivPaper():\n",
    "    def __init__(self, paper):\n",
    "        self.paper = paper\n",
    "\n",
    "        self.essential_metadata_keys = {'abstract', 'arxivId', 'authors', 'citations', 'influentialCitationCount',\n",
    "                                        'doi', 'fieldsOfStudy', 'paperId', 'references',\n",
    "                                        'title', 'topics', 'url', 'venue', 'year'}\n",
    "\n",
    "        self.representational_info_keys = ['abstract', 'authors', 'url', 'year',\n",
    "                                           'fieldsOfStudy', 'numCitations', 'venue', 'numReferences']\n",
    "        \n",
    "        self.check_paper()\n",
    "        self.check_relevant_keys()\n",
    "        self.discard_non_influential_citations()\n",
    "        self.discard_non_influential_references()\n",
    "\n",
    "    def check_paper(self):\n",
    "        if isinstance(self.paper, str):\n",
    "            warnings.warn(\"Paper not present in memory. Extracting Paper MetaData from Semantic Scholar!\")\n",
    "            metadata_extractor = SemanticScholarMetaDataExtractor()\n",
    "            self.paper = metadata_extractor.get_data_json(self.paper)\n",
    "        \n",
    "        elif not isinstance(self.paper, dict):\n",
    "            raise TypeError(\"Paper must be a Dict or an Arxiv Id\")\n",
    "    \n",
    "    def check_relevant_keys(self):\n",
    "        missing_keys = self.essential_metadata_keys.difference(self.paper.keys())\n",
    "        if not missing_keys == set():\n",
    "            error_message = \"The following essential keys are missing from the paper: \" + \\\n",
    "                            \", \".join(missing_keys)\n",
    "            raise KeyError(error_message)\n",
    "        \n",
    "        self.paper['numCitations'] = len(self.paper['citations'])\n",
    "        self.paper['numReferences'] = len(self.paper['references'])\n",
    "\n",
    "    def discard_non_influential_citations(self):\n",
    "        self.paper['citations'] = list(filter(lambda i: i['isInfluential'] is True, self.paper['citations'])) \n",
    "\n",
    "    def discard_non_influential_references(self):\n",
    "        self.paper['references'] = list(filter(lambda i: i['isInfluential'] is True, self.paper['references'])) \n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.paper[key]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr = f\"Paper Title: {self.__getitem__('title')} \\n\\n\"\n",
    "        for idx, key in enumerate(self.representational_info_keys):\n",
    "            if key == 'abstract':\n",
    "                repr += f\"{idx+1}) {'Abstract'}: \\n{self.__getitem__(key)} \\n\\n\"\n",
    "                continue\n",
    "            if key == 'authors':\n",
    "                repr += f\"{idx+1}) {'Authors'}:\\n\"\n",
    "                authors = self.__getitem__(key)\n",
    "                for i, author in enumerate(authors):\n",
    "                    repr += f\"\\t{i+1}) {'Name'}: {author.__getitem__('name')}\\n\"\n",
    "                    repr += f\"\\t{'URL'}: {author.__getitem__('url')}\\n\"\n",
    "                    repr +=\"\\n\"\n",
    "                continue\n",
    "            repr += f\"{idx+1}) {key}: {self.__getitem__(key)} \\n\\n\"\n",
    "        return(repr)\n",
    "    \n",
    "    def get_top_k_citations_information(self, k:int):\n",
    "        if k > self.__getitem__('numCitations'):\n",
    "             warnings.warn(f\"Total citations are {self.__getitem__('numCitations')}. Retrieving all citations\")\n",
    "             k = self.__getitem__('numCitations')\n",
    "\n",
    "        citations = {}\n",
    "        all_citations = self.__getitem__('citations')\n",
    "\n",
    "        info_keys = ['arxivId', 'authors', 'title', 'url','venue', 'year']\n",
    "\n",
    "        i=0\n",
    "        while i < k:\n",
    "            citation = all_citations[i]\n",
    "            if citation['arxivId'] is None:\n",
    "                warnings.warn(f\"The citation at index {i+1} has no Arxiv ID. Skipping this citation.\")\n",
    "                k+=1 \n",
    "                i+=1\n",
    "                continue                   \n",
    "        \n",
    "            citation = {key:val for key, val in citation.items() if key in info_keys}\n",
    "            citations[i+1] = citation\n",
    "            i+=1\n",
    "\n",
    "        return citations\n",
    "\n",
    "    def get_top_k_references_information(self, k:int):\n",
    "        if k > self.__getitem__('numReferences'):\n",
    "             warnings.warn(f\"Total references are {self.__getitem__('numReferences')}. Retrieving all references\")\n",
    "             k = self.__getitem__('numReferences')\n",
    "\n",
    "        references = {}\n",
    "        all_references = self.__getitem__('references')\n",
    "\n",
    "        info_keys = ['arxivId', 'authors', 'title', 'url','venue', 'year']\n",
    "\n",
    "        i=0\n",
    "        while i < k:\n",
    "            reference = all_references[i]\n",
    "\n",
    "            if reference['arxivId'] is None:\n",
    "                warnings.warn(f\"The reference at index {i+1} has no Arxiv ID. Skipping this reference.\")\n",
    "                k+=1\n",
    "                i+=1\n",
    "                continue                   \n",
    "\n",
    "            reference = {key:val for key, val in reference.items() if key in info_keys}\n",
    "            references[i+1] = reference\n",
    "            i+=1\n",
    "\n",
    "        return references\n",
    "\n",
    "    def get_top_k_references_metadata(self, k:int):\n",
    "        reference_papers = {}\n",
    "        references = self.get_top_k_references_information(k)\n",
    "\n",
    "        for i in range(1, len(references)+1):\n",
    "            reference_papers[i] = ArXivPaper(references[i]['arxivId'])\n",
    "\n",
    "        return reference_papers\n",
    "\n",
    "    def get_top_k_citations_metadata(self, k:int):\n",
    "        citation_papers = {}\n",
    "        citations = self.get_top_k_references_information(k)\n",
    "\n",
    "        for i in range(1, len(citations)+1):\n",
    "            citation_papers[i] = ArXivPaper(citations[i]['arxivId'])\n",
    "\n",
    "        return citation_papers\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = ArXivPaper(\"1806.07366\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'arxivId', 'authors', 'citationVelocity', 'citations', 'corpusId', 'doi', 'fieldsOfStudy', 'influentialCitationCount', 'is_open_access', 'is_publisher_licensed', 'paperId', 'references', 'title', 'topics', 'url', 'venue', 'year', 'numCitations', 'numReferences'])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "paper.paper.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "len(paper['citations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "len(paper['references'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ['citationVelocity', 'influentialCitationCount', 'numCitations', 'numReferences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "paper.paper[d[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = paper.get_top_k_citations_information(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{3: {'arxivId': '1906.00586',\n",
       "  'authors': [{'authorId': '52193502', 'name': 'Mitchell Wortsman'},\n",
       "   {'authorId': '143787583', 'name': 'Ali Farhadi'},\n",
       "   {'authorId': '143887493', 'name': 'M. Rastegari'}],\n",
       "  'title': 'Discovering Neural Wirings',\n",
       "  'url': 'https://www.semanticscholar.org/paper/9c48f787f9590fcbad78707419ddfad269102cd3',\n",
       "  'venue': 'NeurIPS',\n",
       "  'year': 2019},\n",
       " 4: {'arxivId': '1905.11602',\n",
       "  'authors': [{'authorId': '2316494', 'name': 'Peter Karkus'},\n",
       "   {'authorId': '145572784', 'name': 'Xiao Ma'},\n",
       "   {'authorId': '1384318941', 'name': 'David Hsu'},\n",
       "   {'authorId': '1709512', 'name': 'L. Kaelbling'},\n",
       "   {'authorId': '1740222', 'name': 'Wee Sun Lee'},\n",
       "   {'authorId': '1388700951', 'name': 'Tomas Lozano-Perez'}],\n",
       "  'title': 'Differentiable Algorithm Networks for Composable Robot Learning',\n",
       "  'url': 'https://www.semanticscholar.org/paper/1e6fbab02acf8baf93e8991a35849cd5b3cbac94',\n",
       "  'venue': 'Robotics: Science and Systems',\n",
       "  'year': 2019},\n",
       " 5: {'arxivId': '1909.13334',\n",
       "  'authors': [{'authorId': '8157979', 'name': 'Zhengdao Chen'},\n",
       "   {'authorId': '49049937', 'name': 'J. Zhang'},\n",
       "   {'authorId': '2877311', 'name': 'MartÃ­n Arjovsky'},\n",
       "   {'authorId': '119267979', 'name': 'L. Bottou'}],\n",
       "  'title': 'Symplectic Recurrent Neural Networks',\n",
       "  'url': 'https://www.semanticscholar.org/paper/60ea57281e62fbf28e2022f085b99a9e01c06162',\n",
       "  'venue': 'ICLR',\n",
       "  'year': 2020}}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation_papers = paper.get_top_k_citations_metadata(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citation_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in paper['citations']:\n",
    "    if i['isInfluential'] == True:\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'arxivId': None,\n",
       " 'authors': [{'authorId': '104314859', 'name': 'Fred Daum'},\n",
       "  {'authorId': '50535618', 'name': 'J. Huang'},\n",
       "  {'authorId': '9130376', 'name': 'A. Noushin'}],\n",
       " 'doi': '10.1117/12.2517980',\n",
       " 'intent': ['background'],\n",
       " 'isInfluential': False,\n",
       " 'paperId': 'd13739de9b7e22eea9ff03c23d322817c14bdfd8',\n",
       " 'title': \"Extremely deep Bayesian learning with Gromov's method\",\n",
       " 'url': 'https://www.semanticscholar.org/paper/d13739de9b7e22eea9ff03c23d322817c14bdfd8',\n",
       " 'venue': 'Defense + Commercial Sensing',\n",
       " 'year': 2019}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "paper.paper['citations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}